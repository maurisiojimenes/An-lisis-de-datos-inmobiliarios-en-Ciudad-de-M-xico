# -*- coding: utf-8 -*-
"""Limpieza de datos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WqqZv2a0RBmxKpOpyh8OC2MZkCtQN_v_

## Limpieza de los datos

A lo largo de este documento vamos a definir el workflow de limpieza de los datos obtenidos por las técnicas de scrapping en los notebooks anteriores, como mencionamos en la primera entrega el objetivo es presentar un Pipeline de KubeFlow deplegable, proponemos el formato de los siguientes notebooks como documentación de los procesos de cada pipeline a fin de facilitar las pruebas de dichos procesos y verificación de su funcionamiento correcto.
"""

import polars as pl
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
from scipy import stats
from statistics import mean

"""Como estamos utilizando datos provenientes de 4 fuentes de información distintas, el primer paso debe ser unificar toda la información en un único conjunto de datos. Esto nos permitirá hacer una correcta exploración inicial de las características así como realizar la limpieza de datos faltantes (ya sea eliminandolos o rellenandolos) y la eliminación de valores atípicos.

A continuación mostramos como unificamos la información extraida de **Century 21** y **Mudafy** en un solo conjunto de datos. En un futuro agregaremos propiedades provenientes de dos sitios web adicionales.

Importamos los datos correspondientes de **Century 21**, estos son importados desde un archivo JSON
"""

df_century21= pl.read_json("DatosOmar.json")

with open("DatosOmar.json", "r") as f:
    data = json.load(f)

"""Creamos el dataframe entrando al diccionario del JSON, tomamos la llave y creamos una columna con sus valores y colocando como nombre la llave."""

datos_omar = []
for key in data.keys():
    df = pl.from_dict(data[key]).transpose()
    df.columns = [key]
    datos_omar.append(df)

"""Importamos los datos correspondientes de **Mudafy**"""

import json
file_path = 'max_properties_encoding.json'
with open(file_path, 'r') as file:
    parsed_data = [json.loads(line) for line in file]

df_mudafy= pl.DataFrame(parsed_data)

"""Cada registro de alguna propiedad independientemente del sitio de donde se haya extraido debe tener las siguientes características:

* **id**: Muestra un identificador numérico con un prefijo que indica de que página proviene ese registro. Por ejemplo C21 indica que se extrajo de Century 21.

* **delegación** : Muestra el nombre en minúsculas y sin acentos de la delegación de la Ciudad de México en donde se encuentra ubicada la propiedad.

* **superficie_de_terreno** : Es un valor flotante que indica en metros cuadrados la superficie total del terreno que está disponible para construir.

* **superficie_construida** : Indica la fracción del terreno (en metro cuadrados) en la que hay edificaciones.

* **recamaras** : Cantidad de habitaciones que posee la propiedad.

* **baños** : Cantidad de baños que posee la propiedad.

* **estacionamiento** : Cantidad de espacios disponibles para aparcar vehículos.

* **precio** : Precio de venta de la propiedad en pesos mexicanos.

* **campos_adicionales** : Informacion variada que en caso de ser requerida se podria utilizar, esta varia dependiendo de la pagina

Aunque los datos extraidos de dos paginas distintas tengan las mismas caracteristicas debemos hacer que los datasets individuales sean compatibles entre si para poder concatenarlos, esto es, verificar que los nombres de las columnas sean iguales y que el tipo de dato de cada una de estas coinciden con sus analogas de los otros datasets, podría ocurrir que superficie_construida sea de tipo entero para **Mudafy** y flotante para **Century 21**. Aquí cambiamos el nombre de la columna precio de venta únicamente por precio:
"""

df_century21=pl.concat(datos_omar,how="horizontal")
df_century21=df_century21.rename({"precio de venta":"precio"})
df_century21

"""Seleccionamos las columnas que comparten ambos dataframes"""

df_century21=df_century21.select(df_mudafy.columns)

df_century21.sample(5)

"""En el dataframe de los datos de **Century 21** hacemos que las columnas recamaras, baños y estacionamiento sean de tipo entero"""

df_century21 = df_century21.with_columns([
    pl.col("recamaras").cast(pl.Int64),
    pl.col("baños").cast(pl.Int64),
    pl.col("estacionamiento").cast(pl.Int64)
])

df_century21

"""id	delegacion	superficie_de_terreno	superficie_construida	recamaras	baños	estacionamiento	precio campos_adicionales




id	delegacion	precio	superficie_de_terreno	superficie_construida	recamaras	baños	estacionamiento	campos_adicionales

En el dataframe de **Mudafy** las columnas superficie_de_terreno , superficie_construida y precio son de tipo entero, hacemos el cambio a flotante
"""

df_mudafy = df_mudafy.with_columns([
    pl.col("superficie_de_terreno").cast(pl.Float64),
    pl.col("superficie_construida").cast(pl.Float64),
    pl.col("precio").cast(pl.Float64)
])


df_mudafy

"""En este caso las columnas de datos adicionales no nos seran utiles por lo que hemos decidido eliminar esta columna de ambos dataframes. Solo queda concatenar:"""

datos=pl.concat([df_century21.drop("campos_adicionales"),df_mudafy.drop("campos_adicionales")],how="vertical")

"""El siguiente dataframe ya esta unificado y con los tipos correctos:



*   String: id y delegación
*   Flotante: precio, superficie_de_terreno y superficie_construida
*   Entero: recamaras, baños y estacionamiento


"""

datos.head()

"""**DE AQUI EN ADELANTE ES GENERAL**

Esta funcion estandariza los nombres de las delegaciones
"""

def Unificar(Nombre):
    conversion = {
        'azcapotzalco': 'Azcapotzalco',
        'benito-juarez': 'Benito Juárez',
        'coyoacan': 'Coyoacán',
        'cuajimalpa-de-morelos': 'Cuajimalpa de Morelos',
        'gustavo-a-madero': 'Gustavo A. Madero',
        'miguel-hidalgo': 'Miguel Hidalgo',
        'tlahuac': 'Tláhuac',
        'iztacalco': 'Iztacalco',
        'iztapalapa': 'Iztapalapa',
        'venustiano-carranza': 'Venustiano Carranza',
        'la-magdalena-contreras': 'La Magdalena Contreras',
        'alvaro-obregon': 'Álvaro Obregón',
        'cuauhtemoc': 'Cuauhtémoc',
        'milpa-alta': 'Milpa Alta',
        'xochimilco': 'Xochimilco',
        'tlalpan': 'Tlalpan',
        'venustiano carranza': 'Venustiano Carranza',
        'xochimilco': 'Xochimilco',
        'coyoacan': 'Coyoacán',
        'azcapotzalco': 'Azcapotzalco',
        'alvaro obregon': 'Álvaro Obregón',
        'iztapalapa': 'Iztapalapa',
        'miguel hidalgo': 'Miguel Hidalgo',
        'benito juarez': 'Benito Juárez',
        'gustavo a. madero': 'Gustavo A. Madero',
        'cuauhtemoc': 'Cuauhtémoc',
        'tlalpan': 'Tlalpan',
        'iztacalco': 'Iztacalco',
        'cuajimalpa': 'Cuajimalpa de Morelos',
        'magdalena contreras': 'La Magdalena Contreras',
        'tlahuac': 'Tláhuac',
        'milpa alta': 'Milpa Alta'
    }
    return conversion.get(Nombre, Nombre)

datos = datos.with_columns(
    pl.col("delegacion").map_elements(Unificar).alias("delegacion")
)
datos["delegacion"].unique()

"""**Eliminación de columnas con gran cantidad de valores nulos**

Calculemos primero el porcentaje de valores faltantes por columna, si alguna de estas tienen un porcentaje alto de valores nulos lo mejor sería deshacernos de ella pues el rellenar una gran cantidad de datos faltantes puede crear mucho ruido en los modelos que utlizaremos en un futuro, si tienen un porcentaje pequeño optaremos por rellenarlos con la medida estadística adecuada.
"""

null_counts = datos.select(pl.col(c).is_null().sum().alias(c) for c in datos.columns)
display(null_counts/datos.height*100)

"""Vemos que los datos nulos en cada columna no superan ni siquiera el 2% he aquí nuestra habilidad para seleccionar fuentes fidedignas y el correcto uso de nuestras técnicas de recopilación.

**Valores atípicos**

Hagamos los graficos de caja para cada caracteristica numerica, para limpiar los valores atipicos hemos decidido utilizar el metodo del rango intercuatilico, veamos entonces los graficos de caja y veamos si 1.5 es el coeficiente adecuado para la eliminacion de datos atipicos o en caso contrario veremos cual es una mejor opcion.
"""

columnas = ['precio',
            'superficie_de_terreno',
            'superficie_construida',
            'recamaras',
            'baños',
            'estacionamiento']
for col in columnas:
    values = datos[col].to_list()
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=values)
    plt.title(f'Boxplot de la columna: {col}')
    plt.ylabel('Valores')
    plt.xlabel(col)
    plt.grid()
    plt.tight_layout()
    plt.show()

"""Podemos ver claramente como todas las caracteristicas tienen valores atipicos, en general podemos decir que el rango de las caracteristicas esta bein condensado, es decir, la gran mayoria de datos se encuentran entre el primer y el tercer cuartil, los datos atipicos se pueden deber a que algunos registros podrian pertenecer a vender condominios o edificios completos, como esos casos no nos interesan podemos borrarlos y no perder informacion.

Las graficas de caja siempre son de utilidad pero algunas veces pareciera que hay una cantidad muy grande de valores atipicos lo que puede resultar confuso, para saber que tanto nos conviene eliminar estos valores sin tener una gran perdida de datos podemos usar los graficos de violin, estos aproximan la funcion de densidad de probabilidad de los datos usando una estimacion de densidad de kernel, el grafico de violin es muy util cuando queremos ver el volumen de datos atipicos y estar tranquilos de no eliminar una cola de las distribucion muy pesada:
"""

columnas = ['precio',
            'superficie_de_terreno',
            'superficie_construida',
            'recamaras',
            'baños',
            'estacionamiento']
colors = sns.color_palette("pastel", len(columnas))

for idx, col in enumerate(columnas):
    values = datos[col].to_list()
    plt.figure(figsize=(8, 6))
    sns.violinplot(x=values, palette=[colors[idx]])
    plt.title(f'Gráfico de Violin de la columna "{col}"', fontsize=12)
    plt.ylabel('Valores', fontsize=10)
    plt.xlabel(col, fontsize=10)
    plt.grid()
    plt.tight_layout()
    plt.show()

"""Notemos por los gráficos anteriores que los datos atípicos no representan una cantidad muy grande con respecto al total de los datos por lo que podemos descartarlos sin mucho problema, en cuanto a los gráficos de caja podemos ver que hay valores extremademente atípicos que aunque sean pocos pueden meter mucho ruido a las medidas estadisticas como la media, en conclusión: Las gráficas de caja nos ayudarán a ver que hay valores atípicos y extremadamente atípicos que debemos descartar y las gráficas de violín nos indican que no tendremos perdida de informacion pues no representan una gran cantidad.

Para eliminar los datos atipicos utilizaremos el metodo del rango intercuartilico , calcularemos los limites superior e inferior de la siguiente manera:

**IQR**=**Q3**-**Q1**

LimiteSuperior=**Q3**+**1.5*IQR**


LimiteSuperior=**Q1**-**1.5*IQR**

Utilizaremos el coeficientes estandar 1.5 , si notamos que es mejor un coeficiente distinto para conservar mas datos o eliminar mas cambiaremos el valor.
"""

#Crearemos esta copia para calcular el IQR de cada caracteristica sin que se vea afectado por la eliminacion de atipicos de otras
Copia=datos.clone()

"""Debemos notar que para la columna de precios hay que analizar los valores atipicos de cada delegacion pues por ejemplo Miguel Hidalgo y Milpa Alta tienen un rango de precios distintos entre si, por lo que algo que se considera atipico en Milpa Alta se considera normal en Miguel Hidalgo y si usamos IQR sin hacer esta distincion corremos el riesgo de eliminar valores totalmente validos"""

plt.figure(figsize=(10,10))
sns.boxplot(data=datos,x="precio",y="delegacion")
plt.show()

#Cramos una funcion que calcula los limites superior e inferior para cada columna
def Limites(datacolumn,z=1.5):
    sorted(datacolumn)
    Q1,Q3=np.nanpercentile(datacolumn,[25,75])
    IQR=Q3-Q1
    lower_range= Q1-(z * IQR)
    upper_range= Q3+(z * IQR)
    return lower_range, upper_range

"""##Eliminacion de atipicos por delegacion"""

#Hacemos un ciclo que entre a limpiar los atipicos de cada delegacion

Delegaciones=datos["delegacion"].unique()

for Del in Delegaciones:
    # Filtrar los precios de la delegación actual
    precio_delegacion = datos.filter(pl.col("delegacion") == Del)["precio"]

    # Calcular los límites para la delegación
    Inf, Sup = Limites(precio_delegacion)

    # Crear la máscara; Permanece si: No es de esa delegacion o esta dentro de los limites
    Mascara = (datos["delegacion"] != Del) | ((datos["precio"] <= Sup) & (datos["precio"] >= Inf))

    # Aplicar la máscara para filtrar los datos
    datos = datos.filter(Mascara)

"""Esta grafica muestra los rangos y los cuartiles del precio por delegacion, se puede apreciar de mejor manera ya que hemos quitado los datos que se salen de lo normal"""

plt.figure(figsize=(10,10))
sns.boxplot(data=datos,x="precio",y="delegacion")
plt.show()

"""## Resto de caracteristicas"""

# Obtener las columnas numéricas del DataFrame (omitimos precio)
Caracteristicas = [col for col in datos.columns if datos[col].dtype in [pl.Int32, pl.Int64, pl.Float32, pl.Float64]][1:]

for Car in Caracteristicas:
    # Filtrar la columna para excluir los valores nulos en los cálculos de límites
    precio_columna = Copia[Car].filter(Copia[Car].is_not_null())

    # Calcular los límites usando la función Limites
    Inf, Sup = Limites(precio_columna)

    # Crear la máscara de validación para esa columna, excluyendo los nulos
    Mascara = (datos[Car] <= Sup) & (datos[Car] >= Inf) | datos[Car].is_null()

    # Aplicar el filtro a los datos
    datos = datos.filter(Mascara)

"""Decidimos usar los coeficientes 1.5 para las superficies y el estacionamiento,  utilizamos un coeficiente 2 para los baños y estacionamientos pues asi tendremos una limite superior de 5 baños o recamaras lo cual es una cantidad no tan común pero las casas de alto valor podrian tenerlos, mas de 5 de estos lo consideramos totalmente atipico. Para ver si estos coeficientes son adecuados ploteemos las distribuciones de los datos, idealemente quisieramos que no haya colas largas."""

import matplotlib.pyplot as plt
import seaborn as sns

medianas=[] #Vamos a guardar las medianas, nos seran utiles despues
Caracteristicas = ["precio",'superficie_de_terreno', 'superficie_construida', 'recamaras', 'baños', 'estacionamiento']
for i in range(len(Caracteristicas)):
    col = Caracteristicas[i]

    seleccion = datos[col].filter(datos[col].is_not_null())
    values = seleccion.to_list()
    mean_val = np.mean(values)  #Calculamos media
    median_val = np.median(values) #Calculamos mediana
    medianas.append((Caracteristicas[i],median_val)) #Guardamos la mediana

    plt.figure(figsize=(8, 6))
    sns.histplot(x=values, color=colors[i])
    plt.title(f'Boxplot de la columna: {Caracteristicas[i]} sin atipicos')
    plt.ylabel('Valores')
    plt.axvline(mean_val, color='red', linestyle='--', label='Media')  #Ploteamos una linea en donde esta la media
    plt.axvline(median_val, color='blue', linestyle='-.', label='Mediana') #Ploteamos una linea en donde esta la mediana
    plt.xlabel(col)
    plt.grid()
    plt.legend()
    plt.tight_layout()
    plt.show()

"""Al eliminar los valores atípicos que obtenemos distribuciones con colas ligeras, parece ser que la elección de los coeficientes fue la correcta. Tenemos el caso particular de la distribución de la superficie del terreno, aquí hay una clara dominación del valor 0 sobre el resto, esto se debe a que las propiedades que viven en edificios como los departamentos no tienen espacio para construir, podemos quitar esta característica pero el resto de los datos que tienen un valor distinto de 0 pueden servir, además el método intercuartilico elimino los valores que podrían considerarse atípicos por lo que debemos poder usar la característica pero teniendo cuidado con el desequilibrio que hay.

**Rellenado de datos faltantes**

Viendo las distribuciones podemos descartar la media en todos los casos, la principal razón es que ninguna de las distribuciones resulta ser simétrica, en estos casos la media no es una buena representación de los datos pues esta más cerca de la cola más larga o sea aquella en la que los valores están cerca de ser atípicos lo que hace que no sea una representación precisa del conjunto. En su lugar tomaremos a la mediana, esta destaca por ser más robusta ante los datos atípicos y colas largas y se suele utilizar cuando los datos no se distribuyen simétricamente pues esta siempre esta en el centro de los datos.

Las medianas de las caracteristicas son:
"""

medianas

"""Rellenamos los datos con sus medianas"""

for col,median in medianas:
    datos=datos.with_columns(pl.when(pl.col(col).is_null()).then(median).otherwise(pl.col(col)).alias(col))

"""Podemos ver que ya no tenemos valores nulos en ninguna columna:"""

null_counts = datos.select(pl.col(c).is_null().sum().alias(c) for c in datos.columns)
display(null_counts/datos.height*100)

Copiadf=datos.clone() #Creamos una copia del original por si acaso

"""Obtenemos el siguiente dataframe:"""

datos

"""Un dataframe de 12,212 registros con 8 caracteristicas, 2 cualitativas y 6 cuantitativas, las columnas ya no tienen valores nulos y los valores atipicos fueron tratados por lo que es un dataset limpio.

Guardamos la información en un archivo JSON
"""

datos.select(["id",	"delegacion"	,"precio"	,"superficie_de_terreno",	"superficie_construida",	"recamaras",	"baños"	,"estacionamiento"]).write_json("datos_limpios.json")

"""Obtenemos una pequeña muestra de los registros"""

datos.sample(n=20, with_replacement=False)

"""# Procesamiento de los datos"""

Procesados=datos.clone()

import polars as pl
import numpy as np

# Definir las funciones
def Normalizar(Xi):
    mu = Xi.mean()
    sigma = Xi.std()
    return (Xi - mu) / sigma

def Escalado(Xi):
    minimo = Xi.min()
    maximo = Xi.max()
    return (Xi - minimo) / (maximo - minimo)

# Aplicar las funciones a las columnas de 'Procesados'
for col in ["precio","superficie_de_terreno","superficie_construida",	"recamaras",	"baños"	,"estacionamiento"]:
    # Normalizar la columna
    Procesados = Procesados.with_columns(
        Normalizar(pl.col(col)).alias(f"{col}_normalizado")
    )

    # Escalar la columna
    Procesados = Procesados.with_columns(
        Escalado(pl.col(col).alias(f"{col}_escalado")
    ))

    #Escalar y normalizar la columna
    Procesados=Procesados.with_columns(
        Escalado(pl.col(col)).alias(f"{col}_EN")
    )
    Procesados=Procesados.with_columns(
        Normalizar(pl.col(f"{col}_EN")).alias(f"{col}_EN")
    )

# Mostrar el resultado
#Procesados = Procesados.drop(["precio","superficie_de_terreno","superficie_construida",	"recamaras",	"baños"	,"estacionamiento"])
print(Procesados)

Procesados[["precio","precio_escalado","precio_normalizado","precio_EN"]]

"""# Analisis de correlacion"""

sns.pairplot(datos.to_pandas(), hue="delegacion", diag_kind="kde", palette="Set2")

"""Hacemos una matriz de correlacion y nu mapa de calor"""

# Convertir solo las columnas numéricas a pandas
DatosPandas = datos.select(pl.col(pl.NUMERIC_DTYPES)).to_pandas()

# Calcular la matriz de correlación con pandas
correlacion = DatosPandas.corr()
print(correlacion)
# Crear un mapa de calor con Seaborn
sns.heatmap(correlacion, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Matriz de Correlación")
plt.show()

"""# Mapas de la cdmx"""

#Ploteo de CDMX por colores dependiendo de cierta caracteristica

import geopandas as gpd

def Unificar(Nombre):

    Convertir = {
        'azcapotzalco': 'Azcapotzalco',
        'benito-juarez': 'Benito Juárez',
        'coyoacan': 'Coyoacán',
        'cuajimalpa-de-morelos': 'Cuajimalpa de Morelos',
        'gustavo-a-madero': 'Gustavo A. Madero',
        'miguel-hidalgo': 'Miguel Hidalgo',
        'tlahuac': 'Tláhuac',
        'iztacalco': 'Iztacalco',
        'iztapalapa': 'Iztapalapa',
        'venustiano-carranza': 'Venustiano Carranza',
        'la-magdalena-contreras': 'La Magdalena Contreras',
        'alvaro-obregon': 'Álvaro Obregón',
        'cuauhtemoc': 'Cuauhtémoc',
        'milpa-alta': 'Milpa Alta',
        'xochimilco': 'Xochimilco',
        'tlalpan': 'Tlalpan'
    }

    return Convertir.get(Nombre, Nombre)

#Enviar (dataframe,caracteristica)
def Mapa(df,caracteristica="precio"):

  #Convertimos el dataframe en uno de pandas si es que no lo está
  MapaDatos=df.to_pandas()

  #Importamos el geo dataframe del archivo shp
  delegaciones_mapa = "/content/poligonos_alcaldias_cdmx.shp"
  gdf_delegaciones = gpd.read_file(delegaciones_mapa)

  #Hacemos que las columnas de nuestro dataframe coincidan con las del geo df
  MapaDatos["delegacion"]=MapaDatos["delegacion"].apply(Unificar)

  # Hacemos un dataframe que tenga calculado el estadistico que queramos utilizar en el mapa, trae la mediana por defecto por los atipicos
  dfAuxiliar = MapaDatos.groupby('delegacion')[caracteristica].median().reset_index()

  #Unimos el dfAuxiliar con el geo dataframe (Revisa que el nombre de la columna de tus delegaciones se llame delegacion o le cambias aqui)
  gdf_delegaciones = gdf_delegaciones.merge(dfAuxiliar, left_on='NOMGEO', right_on='delegacion', how='left')

  #Ploteamos el mapa (Por defecto esta en la paleta viridis)
  fig, ax = plt.subplots(1, 1, figsize=(10, 10))
  gdf_delegaciones.plot(column=caracteristica,ax=ax, legend=True,
                        legend_kwds={'label': "Precio medio por delegación",
                                    'orientation': "vertical"},
                        cmap='viridis')  # Elige el cmap para los colores

  plt.title(f"Mapa de {caracteristica} Medio por Delegación en CDMX")
  plt.show()

Mapa(datos,"superficie_construida")

